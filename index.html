<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MVL Challenge</title>

    <meta name="description" content="We introduce the first multi-view layout estimation challenge, where the goal is to encourage the community to develop solutions for room layout estimation by using only sequences of registered 360-images as unique input.">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/boostrap.min.css">

    <style>
        a {
            font-weight: bold;
        }
        
        ol {
            counter-reset: list;
        }
        
        ol>li {
            list-style: none;
            position: relative;
        }
        
        ol>li:before {
            counter-increment: list;
            content: "[" counter(list) "] ";
            position: absolute;
            left: -1.4em;
        }
    </style>

    <link rel="icon" type="image/png" sizes="32x32" href="img/favicon.png">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">

    <!--  Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Multi-View Layout Challenge">
    <meta property="og:site_name" content="Multi-View Layout Challenge">
    <meta property="og:description" content="We introduce the first multi-view layout estimation challenge, where the goal is to encourage the community to develop solutions for room layout estimation by using only sequences of registered 360-images as unique input.">
    <meta property="og:url" content="https://mvlchallenge.github.io/">
    <meta property="og:image" content="https://evalai.s3.amazonaws.com/media/logos/139ecc86-5393-4535-b564-3cfdfc6a4877.jpg">

    <!-- Twitter -->
    <meta name="twitter:card" content="website">
    <meta name="twitter:title" content="Multi-View Layout Challenge">
    <meta name="twitter:description" content="We introduce the first multi-view layout estimation challenge, where the goal is to encourage the community to develop solutions for room layout estimation by using only sequences of registered 360-images as unique input.">
    <meta name="twitter:url" content="https://mvlchallenge.github.io/">
    <meta name="twitter:image" content="https://evalai.s3.amazonaws.com/media/logos/139ecc86-5393-4535-b564-3cfdfc6a4877.jpg">
</head>

<body>
    <div class="row" style="background-image: url('img/bg.gif'); padding-top: 8%; padding-bottom: 8%;">
        <h1 class="col-md-12 text-center">
            <b style="color: white;">Multi-View Layout Challenge</b></br>
            <small style="color: white;">
                in conjunction with CVPR'23 OmniCV Workshop
            </small>
        </h1>
    </div>

    <div class="container" id="main">
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    We introduce the first multi-view layout estimation challenge,
                    where the goal is to encourage the community to develop solutions 
                    for room layout estimation which only use sequences of registered 360-images as unique input.
                </p>
                <p class="text-justify">
                    Unlike most existing layout estimation datasets, 
                    our challenge provides abundant multi-view images within more complex scenes with larger rooms,
                    more rooms per scene, and more non-Manhattan spaces, which brings 
                    new challenges for room layout estimation.
                </p>
    
                <p class="text-justify">
                    This challenge is part of the <a href="https://sites.google.com/view/omnicv2022">Omnidirectional Computer Vision (OmniCV)</a> workshop at <a href="https://cvpr2023.thecvf.com/">CVPR'23</a>.
                </p>
                <!-- <img width="500" height="500" src="https://evalai.s3.amazonaws.com/media/logos/139ecc86-5393-4535-b564-3cfdfc6a4877.jpg" class="img-responsive center-block" alt="overview"> -->
                <img width="500" height="500" src="img/mvl.gif" class="img-responsive center-block" alt="overview">
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dataset
                </h3>
                <p class="text-justify">
                    We include the following two datasets in this competition.
                </p>
                <ul>
                    <li><b>MP3D-FPE</b> is a synthetic dataset proposed in <a href="https://arxiv.org/abs/2112.06180">[1] 360-DFPE</a> with 50 scenes and 687 rooms in total.
                    <!-- for the purpose of floor plan estimation.  MP3D-FPE contains 50 scenes and 687 rooms in total.   -->
                    <!-- For each scene, it provides the point cloud, sequence of panorama images with depth, camera pose information, and the floor plan annotation.  -->
                        For this competition, only RGB images together with its registered camera poses are released. 
                        In total, we include 20k and 2.2k samples for training and testing, respectively.
                    </li>
                    <li><b>HM3D-MVL</b> is a newly collected dataset using the
                        <a href="https://aihabitat.org/">[2] Habitat</a> simulator. 
                        Similar to MP3D-FPE, we imitate users scanning behavior to collect more realistic camera movements. 
                        In total, we include 20k and 2.2k samples for training and testing, respectively.
                    </li>
                </ul>
                <p class="text-justify">
                    We provide a <a href="git@github.com:EnriqueSolarte/mvl_toolkit.git">toolkit</a> to make data processing easier, allowing participants to 
                    download and use dataset splits, register frames by camera poses, and format 
                    the layout estimation results for the challenge submission.</p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Challenge
                </h3>
                <ul>
                    <li>Our competition will be held in
                        <a href="https://eval.ai/web/challenges/challenge-page/1906/overview">EvalAI</a>.</li>
                    <li>
                        Participants will have access to three sets of data: training, warm-up, and challenge set. All sets will be provided in the form of multiple 360-images per room, registered by camera poses, but without layout annotations.
                    </li>
                    <!-- <li>We will release the warm-up set (about 1% of the challenge set) and the baseline, <a href="https://arxiv.org/abs/2210.12935">[3] 360-MLC</a>, along with the competition, while the challenge set will be released later in the test phase.</li> -->
                    <li>The warm-up set is composed of 1% of the challenge set. This set will be released for evaluation with unlimited number of submissions.
                    <li>The challenge set will be released later in the Challenge Phase Open.</li> 
                    <li>We release and evaluate the <a href="https://arxiv.org/abs/2210.12935">[3] 360-MLC</a> as baseline along with the competition. </li> 
                    <li>The winner will be selected by the best average score evaluated on the challenge set. For more details please refer to <a href="#evaluation">Evaluation</a>.</li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Rules
                </h3>
                <ul>
                    <li>To participate in the competition, individuals and teams (unlimited size) should fill out this registration <a href="">form</a>.
                        <li>Anyone can access the datasets and competition, but only those registered can participate to receive the official award at the <a href="https://sites.google.com/view/omnicv2022"> CVPR23 OmniCV workshop</a>.</li>
                        <li>In total, each person/team is limited up to 10 submissions per day during the challenge period.</li>
                        <li>Each person/team has unlimited number of submission using the warm-up set.</li>
                        <li>External data, freely and publicly available, is allowed, including pre-trained models.</li>
                        <li>There are no limits on training time or network capacity.</li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Timeline
                </h3>
                <ul>
                    <i>(This schedule presents tentative dates only)</i>
                    <li>Competition Begins - <b>March 20, 2023</b> <i></i></li>
                    <li>Warm-up Phase Open - <b>March 20, 2023</b></li>
                    <li>Challenge Phase Open - <b>April 3, 2023.</b></li>
                    <li>Challenge Phase Deadline - <b>June 3, 2023</b></li>
                    <li>Winner informed - <b>June 6, 2023</b></li>
                    <li>Winner presentation - <b>June 19, 2023</b></li>
                    <li>All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.</li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Community
                </h3>
                Join our <a href="">https://mvl-challenge.slack.com/join/shared_invite/zt-1m95ef0hy-ViG7fSeTt1EqiosRlZoDvQ#/shared-invite/email</a> for public queries and discussion, for private queries, please send an email to <br>
                <span style="font-family: 'Courier New', monospace;">mvl.challenge[at]gmail[dot]com</span>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    References
                </h3>
                <ol>
                    <li>
                        Solarte, B., Liu, Y. C., Wu, C. H., Tsai, Y. H., & Sun, M. 360-DFPE: Leveraging Monocular 360-Layouts for Direct Floor Plan Estimation. IEEE Robotics and Automation Letters, 2022.
                    </li>
                    <li>
                        Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., ... & Batra, D. Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
                    </li>
                    <li>
                        Solarte, B., Wu, C. H., Liu, Y. C., Tsai, Y. H., & Sun, M. 360-MLC: Multi-view Layout Consistency for Self-training and Hyper-parameter Tuning. In Advances in Neural Information Processing Systems, 2022.
                    </li>
                </ol>
            </div>
        </div>

        <br>

    </div>
</body>

</html>